{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a263efd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "590992de",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560102dd",
   "metadata": {},
   "source": [
    "### Building Your Model\n",
    "    The steps to building and using a model are:\n",
    "    1.Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n",
    "    2.Fit: Capture patterns from provided data. This is the heart of modeling.\n",
    "    3.Predict: Just what it sounds like\n",
    "    4.Evaluate: Determine how accurate the model's predictions are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421eb548",
   "metadata": {},
   "source": [
    "### Underfitting & Overfitting\n",
    "    1）Overfitting - Tree depth is too large, only a few of training data in each leaf causing unreliable predictions on new data\n",
    "    2）Underfitting - Tree depth is too shallow, too large volume of training data in each leaf causing failure in capturing important distinctions and patterns on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4fef",
   "metadata": {},
   "source": [
    "<img src=\".\\pic\\underfitting_overfitting.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9e59c",
   "metadata": {},
   "source": [
    "But the **max_leaf_nodes** argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n",
    "We can use a utility function to help compare MAE scores from different values for **max_leaf_nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6300d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_errorfrom sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "model.fit(train_X, train_y)\n",
    "preds_val = model.predict(val_X)\n",
    "mae = mean_absolute_error(val_y, preds_val)\n",
    "return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f7768",
   "metadata": {},
   "source": [
    "We can use a for-loop to compare the accuracy of models built with different values for **max_leaf_nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12beed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare MAE with differing values of max_leaf_nodesfor max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34493db8",
   "metadata": {},
   "source": [
    "Max leaf nodes: 5 Mean Absolute Error: 347380  \n",
    "\n",
    "Max leaf nodes: 50 Mean Absolute Error: 258171  \n",
    "\n",
    "Max leaf nodes: 500 Mean Absolute Error: 243495  \n",
    "\n",
    "Max leaf nodes: 5000 Mean Absolute Error: 254983"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd977a5",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba676017",
   "metadata": {},
   "source": [
    "The random forest uses many trees, and it makes a prediction by **averaging the predictions of each component tree**. It generally has **much better predictive accuracy than a single decision tree** and it works well with default parameters. \n",
    "Random Forests can significantly solve over-fitting problem by  \n",
    "    1.  Ensemble Learning - by combining multiple decision trees to make predictions by averaging out the errors and capture more representive patterns  \n",
    "    2.  Random Feature Selection - each tree is built uing a random subset of features at each split preventing from memorizing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6871324",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb73c5",
   "metadata": {},
   "source": [
    "#### 1) A Simple Option: Drop Columns with Missing Values   \n",
    "    Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach. \n",
    "#### 2) A Better Option: Imputation  \n",
    "    Fill in missing values with mean value\n",
    "\n",
    "#### 3) An Extension To Imputation  \n",
    "\n",
    "    In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0517c08",
   "metadata": {},
   "source": [
    "<img src=\".\\pic\\imputation.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e06c7a",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c727b6",
   "metadata": {},
   "source": [
    "#### 1) Drop Categorical Variables  \n",
    "    The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "#### 2) Ordinal Encoding  \n",
    "    This approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "#### 3) One-Hot Encoding\n",
    "    **One-hot encoding** creates new columns indicating the presence (or absence) of each possible value in the original data. To understand this, we'll work through an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115c9a6",
   "metadata": {},
   "source": [
    "<img src=\".\\pic\\one_host_encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb03eb",
   "metadata": {},
   "source": [
    "In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. *Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data   \n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
