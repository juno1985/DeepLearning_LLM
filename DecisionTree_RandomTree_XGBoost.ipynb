{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a263efd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "590992de",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560102dd",
   "metadata": {},
   "source": [
    "### Building Your Model\n",
    "    The steps to building and using a model are:\n",
    "    1.Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n",
    "    2.Fit: Capture patterns from provided data. This is the heart of modeling.\n",
    "    3.Predict: Just what it sounds like\n",
    "    4.Evaluate: Determine how accurate the model's predictions are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421eb548",
   "metadata": {},
   "source": [
    "### Underfitting & Overfitting\n",
    "    1）Overfitting - Tree depth is too large, only a few of training data in each leaf causing unreliable predictions on new data\n",
    "    2）Underfitting - Tree depth is too shallow, too large volume of training data in each leaf causing failure in capturing important distinctions and patterns on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd4fef",
   "metadata": {},
   "source": [
    "<img src=\".\\pic\\underfitting_overfitting.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b37dc",
   "metadata": {},
   "source": [
    "But the **max_leaf_nodes** argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n",
    "We can use a utility function to help compare MAE scores from different values for **max_leaf_nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_errorfrom sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "model.fit(train_X, train_y)\n",
    "preds_val = model.predict(val_X)\n",
    "mae = mean_absolute_error(val_y, preds_val)\n",
    "return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb65ca87",
   "metadata": {},
   "source": [
    "We can use a for-loop to compare the accuracy of models built with different values for **max_leaf_nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf980e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare MAE with differing values of max_leaf_nodesfor max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd352e8f",
   "metadata": {},
   "source": [
    "Max leaf nodes: 5 Mean Absolute Error: 347380  \n",
    "\n",
    "Max leaf nodes: 50 Mean Absolute Error: 258171  \n",
    "\n",
    "Max leaf nodes: 500 Mean Absolute Error: 243495  \n",
    "\n",
    "Max leaf nodes: 5000 Mean Absolute Error: 254983"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b1e8e",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a30744",
   "metadata": {},
   "source": [
    "The random forest uses many trees, and it makes a prediction by **averaging the predictions of each component tree**. It generally has **much better predictive accuracy than a single decision tree** and it works well with default parameters. \n",
    "Random Forests can significantly solve over-fitting problem by  \n",
    "    1.  Ensemble Learning - by combining multiple decision trees to make predictions by averaging out the errors and capture more representive patterns  \n",
    "    2.  Random Feature Selection - each tree is built uing a random subset of features at each split preventing from memorizing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4625ee",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79a6a5",
   "metadata": {},
   "source": [
    "#### 1) A Simple Option: Drop Columns with Missing Values   \n",
    "    Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach. \n",
    "#### 2) A Better Option: Imputation  \n",
    "    Fill in missing values with mean/median/most_frequent/constant(replace with `fill_value`)  \n",
    "    <class sklearn.impute.SimpleImputer>\n",
    "\n",
    "#### 3) An Extension To Imputation  \n",
    "\n",
    "    In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba7d5d6",
   "metadata": {},
   "source": [
    "<img src=\".\\pic\\imputation.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c66ed",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5b01f",
   "metadata": {},
   "source": [
    "#### 1) Drop Categorical Variables  \n",
    "    The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.\n",
    "#### 2) Ordinal Encoding  \n",
    "    This approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "#### 3) One-Hot Encoding\n",
    "    **One-hot encoding** creates new columns indicating the presence (or absence) of each possible value in the original data. To understand this, we'll work through an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431f8bb",
   "metadata": {},
   "source": [
    "<img src=\".\\pic\\one_host_encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c839900-78e6-4118-8d24-57a67903741c",
   "metadata": {},
   "source": [
    "There are 2 ways to apply one-hot encoding:\n",
    "- sklearn.preprocessing.OneHotEncoder\n",
    "- pandas.get_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a0ba4",
   "metadata": {},
   "source": [
    "In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. *Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data*   \n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than **15 different values**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd870db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f5116c-cd36-4cc4-b3d7-11b4d8e5c54d",
   "metadata": {},
   "source": [
    "**Tips** Columns can vary after applying one-hot encoding to training set and validation set due to different categorial values presence.  \n",
<<<<<<< HEAD:DecisionTree_RandomTree_XGBoost.ipynb
    "To align with training data and just drop the categories in validation data which not present in training data:  \n",
    "`X_train, X_valid = X_train.align(X_valid, join='left', axis=1)`"
=======
    "To align with training data and just drop the categories in validation data and test data not present in training data,you can use `pandas.align` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b69670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the data (to shorten the code, we use pandas)\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_valid = pd.get_dummies(X_valid)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1)"
>>>>>>> ecbd85c88ddb4e1afbc76fcea249f82a1c32298d:note.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e2f5a-cb42-4d58-abca-a3aca629ea44",
   "metadata": {},
   "source": [
    "#### tips - Ordinal Encoding\n",
    "Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.  \n",
    "This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue. For instance, you can write a custom ordinal encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns.  \n",
    "Run the code cell below to save the problematic columns to a Python list bad_label_cols. Likewise, columns that can be safely ordinal encoded are stored in good_label_cols.  abel_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072781fc-556f-46b6-98c8-62bbc0b001c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Categorical columns in the training data\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "###### Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        # Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d82aaa-83da-4858-b062-b2a16153bb29",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "**1.Cleaner Code:** Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.  \n",
    "**2.Fewer Bugs:** There are fewer opportunities to misapply a step or forget a preprocessing step.  \n",
    "**3.Easier to Productionize:** It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.  \n",
    "**4.More Options for Model Validation:** You will see an example in the next tutorial, which covers cross-validation.ion.on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34a433-9b7a-424f-95cf-9839df94ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis=1)\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c745b7-1ca7-4fe5-8ac8-1a8385e7cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d9d39-fd33-4ac8-811e-808c361767ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94764288-3871-4621-abcf-a6058a58049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e4fd0-e6d3-4a51-afa6-1444cd337484",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdea8ec-78bc-474a-8e3e-fafc7dfaac40",
   "metadata": {},
   "source": [
    "#### When should you use cross-validation?  \n",
    "- if your model takes a couple minutes or less to run, it's probably worth switching to cross-validation.\n",
    "- Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each experiment yields the same results, a single validation set is probably sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2e365-2382-42e9-bbe1-4119861c7b30",
   "metadata": {},
   "source": [
    "#### What is cross-validation?\n",
    "In cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality.\n",
    "\n",
    "For example, we could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 **\"folds\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae8251-6d73-4c2f-9b83-9556b68b9125",
   "metadata": {},
   "source": [
    "<img src=\".\\pic\\cross_validation.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb371ab-e998-4a5b-ae96-dc0299b629da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5, #we set the number of folds with the cv parameter.\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37fa3c-8501-4e99-a836-429e829ea8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98629349-e128-4a91-a04f-5b444c5aa88a",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dc570-7384-42ec-b89e-9625d8bf8473",
   "metadata": {},
   "source": [
    "**XGBoost** stands for **extreme gradient boosting**, which is an implementation of gradient boosting with several additional features focused on performance and speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466656e-c4cf-46e0-bd34-1cfbcdbef94f",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e8572-e881-4356-a36d-c750120af6ee",
   "metadata": {},
   "source": [
    "**n_estimators** specifies how many times to go through the modeling cycle described above. `It is equal to the number of models that we include in the ensemble.`\n",
    "\n",
    "- Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data.\n",
    "- Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data (which is what we care about)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c48cde-fb95-45e0-a215-e0ae92a906ad",
   "metadata": {},
   "source": [
    "Typical values range from **100-1000**, though this depends a lot on the **learning_rate** parameter discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c763c-c923-4280-9540-c897e6e5255f",
   "metadata": {},
   "source": [
    "**early_stopping_rounds** offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators.  \n",
    "Setting early_stopping_rounds=5 is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67367d-7fa1-4466-a4ca-c1c65ccd5ca0",
   "metadata": {},
   "source": [
    "**learning_rate**\n",
    "Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in.  \n",
    "This means each tree we add to the ensemble helps us less. So, we can set a higher value for n_estimators without overfitting. If we use early stopping, the appropriate number of trees will be determined automatically.  \n",
    "In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0cd1b-7468-4728-ad82-c2a8220e0c7a",
   "metadata": {},
   "source": [
    "**n_jobs**\n",
    "On larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter `n_jobs equal to the number of cores` on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "`The resulting model won't be any better`, so micro-optimizing for fitting time is typically nothing but a distraction. But, it's useful in large datasets where you would otherwise spend a long time waiting during the fit command.(`只是fit的时间变短了，并不影响model的准确度`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4b791-a674-4f2f-a04b-e1576923802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=500, learning_rate=0.05, n_jobs=4)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid, y_valid)],\n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd58f0-1a7b-4515-b68a-81e73f190d6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb278b-110e-41f2-8279-bb67769d428d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991ca1b-5f41-46aa-a26a-d80b25b18e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36a21e-27b0-44e5-8d15-5810e44af7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
